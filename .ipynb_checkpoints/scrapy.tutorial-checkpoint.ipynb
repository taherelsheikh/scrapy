{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Scrapy Tutorial</h2></center><br>\n",
    "These are markdowns based on this https://learn.scrapinghub.com/scrapy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terminal:\n",
    "> * **scrapy shell http://quotes.toscrape.com**\t(print response.text\tprints the whole page's content)\n",
    "* **scrapy runspider quotes.py** (this will print it in the terminal)\n",
    "* **scrapy runspider quotes.py -o items.json** (this will save it to a JSON file)\n",
    "* **response.css('small.author')**\t\n",
    "* **response.css('small.author').extract()**  (prints a list)\n",
    "* **response.css('small.author::text').extract()**\t(prints a list of strings of all tags. tags not included)\n",
    "* **response.css('small.author::text')[2].extract()** (indexs the 2nd element in the list)\n",
    "* **response.css('small.author::text').extract_first()** (prints a string)\n",
    "* **response.css('span.text::text').extract_first()** (same thing for a real example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video 2 - collect a specifci item from a page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class QuotedSpider(scrapy.Spider):\n",
    "    # Name of spider\n",
    "    name = 'quotes'\n",
    "    allowed_domains = ['toscrape.com']\n",
    "    start_urls = ['http://quotes.toscrape.com/random']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        self.log('i just visted' + response.url)\n",
    "        # Create mutiple diconaries\n",
    "        yield {\n",
    "            'author_name': response.css('small.author::text').extract_first(),\n",
    "             'text': response.css('span.text::text').extract_first(),\n",
    "             # This is a list of tags\n",
    "              'tags': response.css('a.tag::text').extract()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terminal: <br>\n",
    "> * **scrapy runspider quotes.py** (this will print it in the terminal)\n",
    "* **scrapy runspider quotes.py -o items.json** (this will save it to a JSON file)\n",
    "* **more items.json** (this will print the json file into the terminal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video 3 - collect mutiple items from a single page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terminal:\n",
    "> * **quote = response.css('div.quote')[0] **\n",
    "* **quote.css('small.author::text').extract()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a for loop \n",
    "for quote in response.css('div.quote'):\n",
    "    item = {\n",
    "        'author_name': quote.css('small.author::text').extract_first()\n",
    "        'text': quote.css('span.text::text').extract_first(),\n",
    "        'tags': quote.css('a.tag::text').extract(),\n",
    "    }\n",
    "    print (item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class QuotedSpider(scrapy.Spider):\n",
    "    # Name of spider\n",
    "    name = 'quotes'\n",
    "    allowed_domains = ['toscrape.com']\n",
    "    start_urls = ['http://quotes.toscrape.com/random']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        self.log('i just visted' + response.url)\n",
    "        # create a for loop \n",
    "        for quote in response.css('div.quote'):\n",
    "            item = {\n",
    "                 'author_name': quote.css('small.author::text').extract_first()\n",
    "                 'text': quote.css('span.text::text').extract_first(),\n",
    "                 'tags': quote.css('a.tag::text').extract(),\n",
    "                   }\n",
    "             yield item     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video 4 - scrape from mutiple pages also known as pagination links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terminal:\n",
    ">- **response.css('li.next > a').extract_first()**\n",
    "- this will print > href=\"/page/2/\">Next span aria-hidden=\"true\">\\u2192</span</a\n",
    "\n",
    "Since we only want the href \n",
    ">- ***response.css('li.next > a::attr(href)').extract_first()***\n",
    "- this will print > u'/page/2/'\n",
    "\n",
    "join the url we passed in the begining with the new one\n",
    ">- response.urljoin(next_page_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class QuotedSpider(scrapy.Spider):\n",
    "    # Name of spider\n",
    "    name = 'quotes'\n",
    "    allowed_domains = ['toscrape.com']\n",
    "    start_urls = ['http://quotes.toscrape.com/random']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        self.log('i just visted' + response.url)\n",
    "        # create a for loop \n",
    "        for quote in response.css('div.quote'):\n",
    "            item = {\n",
    "                 'author_name': quote.css('small.author::text').extract_first()\n",
    "                 'text': quote.css('span.text::text').extract_first(),\n",
    "                 'tags': quote.css('a.tag::text').extract(),\n",
    "                   }\n",
    "             yield item\n",
    "                \n",
    "        # Get the new link for the next page\n",
    "        next_page_url = response.css('li.next > a::attr(href)').extract_first()\n",
    "        # Check and see if theres a next page\n",
    "        if next_page_url:\n",
    "            next_page_url = response.urljoin(next_page_url)\n",
    "            yield scrapy.Request(url=next_page_url, callback=self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video 5 - scrapping details per page example about link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terminal:\n",
    "> * **fetch('httpe://hfwhfuhwf.com')** this will get a new url as oppose to closing the shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class QuotedSpider(scrapy.Spider):\n",
    "    # Name of spider\n",
    "    name = 'authors'\n",
    "    start_urls = ['http://quotes.toscrape.com/random']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        urls = response('div.quote > span > a::attr(href)').extract()\n",
    "        for url in urls:\n",
    "            url = response.urljoin(url)\n",
    "            url = scrapy.Request(url=url, callback=self.parse_details)\n",
    "   \n",
    "        next_page_url = response.css('li.next > a::attr(href)').extract_first()\n",
    "        # Check and see if theres a next page\n",
    "        if next_page_url:\n",
    "            next_page_url = response.urljoin(next_page_url)\n",
    "            yield scrapy.Request(url=next_page_url, callback=self.parse)\n",
    "    \n",
    "    def parse_details(self, response):\n",
    "        yield {\n",
    "            'name': response.css('h3.author-title::text').extract_first(),\n",
    "             'birth_date': response.css('span.author-born-date::text').extract_first(),  \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
